{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# The input file is read and it is splitted into X and Y:\n\ntrain_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data  = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\nY = train_data[\"SalePrice\"]\nX = train_data.drop([\"SalePrice\",\"Id\"], axis=1)\nY =  np.log1p(Y)\nIdTest = test_data.Id\ntest_data = test_data.drop(\"Id\", axis = 1)\n\nprint (X.shape)\nprint (test_data.shape)\n#print (train_data.iloc[:,6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the columns of the original dataset were analysed and the following conclusions were drawn:\n\n# For some columns we just dont have enough data so as the model could learn what each categorical possibility means in terms of price. For example the neighborhood. This kind of categorical features that are splitted into several columns just add noise to the data if you dont have enough examples.\nX         = X.drop        ([\"Exterior1st\",\"Exterior2nd\",\"Condition1\",\"Condition2\"], axis =1)\ntest_data = test_data.drop([\"Exterior1st\",\"Exterior2nd\",\"Condition1\",\"Condition2\"], axis =1)\n\n# For MSSubClass, there is no point in treating the categories as numbers, given that a bigger or smaller number doesn't mean more or less value:\nX[\"MSSubClass\"]          = X[\"MSSubClass\"].astype(str)\ntest_data [\"MSSubClass\"] = test_data[\"MSSubClass\"].astype(str)\n\n# Other columns express the quality of some part of the house. That should not be included in the get_dummies function.\n# It would be desirable to transform the scale from Excellent to Poor into a numerical scale, such as from 5 to 1.\nfromStringToInt = {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,np.nan:0}\nqualityColumns = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"HeatingQC\",\"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]\nfor column in qualityColumns:\n    X[column].replace(fromStringToInt, inplace=True)\n    test_data[column].replace(fromStringToInt, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some of the columns need a specific dictionary to convert from String to Categorical Quality Numbers:\nfromStringToIntLotShape = {\"Reg\":4,\"IR1\":3,\"IR2\":2,\"IR3\":1,np.nan:0}\nX[\"LotShape\"].replace(fromStringToIntLotShape, inplace=True)\ntest_data[\"LotShape\"].replace(fromStringToIntLotShape, inplace=True)\n\nfromStringToIntUtilities = {\"AllPub\":4,\"NoSewr\":3,\"NoSeWa\":2,\"ELO\":1,np.nan:0}\nX[\"Utilities\"].replace(fromStringToIntUtilities, inplace=True)\ntest_data[\"Utilities\"].replace(fromStringToIntUtilities, inplace=True)\n\nfromStringToIntLandSlope = {\"Gtl\":3,\"Mod\":2,\"Sev\":1,np.nan:0}\nX[\"LandSlope\"].replace(fromStringToIntLandSlope, inplace=True)\ntest_data[\"LandSlope\"].replace(fromStringToIntLandSlope, inplace=True)\n\nfromStringToIntBsmtFinType = {\"GLQ\":6,\"ALQ\":5,\"BLQ\":4,\"Rec\":3,\"LwQ\":2,\"Unf\":1,np.nan:0}\nX[\"BsmtFinType1\"].replace(fromStringToIntBsmtFinType, inplace=True)\ntest_data[\"BsmtFinType1\"].replace(fromStringToIntBsmtFinType, inplace=True)\nX[\"BsmtFinType2\"].replace(fromStringToIntBsmtFinType, inplace=True)\ntest_data[\"BsmtFinType2\"].replace(fromStringToIntBsmtFinType, inplace=True)\n\nfromStringToIntFunctional = {\"Typ\":8,\"Min1\":7,\"Min2\":6,\"Mod\":5,\"Maj1\":4,\"Maj2\":3,\"Sev\":2,\"Sal\":1,np.nan:0}\nX[\"Functional\"].replace(fromStringToIntFunctional, inplace=True)\ntest_data[\"Functional\"].replace(fromStringToIntFunctional, inplace=True)\n\nfromStringToIntGarageFinish = {\"Fin\":3,\"RFn\":2,\"Unf\":1,np.nan:0}\nX[\"GarageFinish\"].replace(fromStringToIntGarageFinish, inplace=True)\ntest_data[\"GarageFinish\"].replace(fromStringToIntGarageFinish, inplace=True)\n\nfromStringToIntPavedDrive = {\"Y\":3,\"P\":2,\"N\":1,np.nan:0}\nX[\"PavedDrive\"].replace(fromStringToIntPavedDrive, inplace=True)\ntest_data[\"PavedDrive\"].replace(fromStringToIntPavedDrive, inplace=True)\n\nfromStringToIntFence = {\"GdPrv\":4,\"MnPrv\":3,\"GdWo\":2,\"MnWw\":1,np.nan:0}\nX[\"Fence\"].replace(fromStringToIntFence, inplace=True)\ntest_data[\"Fence\"].replace(fromStringToIntFence, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (X.shape)\nprint (test_data.shape)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before splitting categorical variables, let's get only the numerical columns and check which ones are skewed:\nnameOfNumericColumns = X.dtypes[X.dtypes!=\"object\"].index\nskewedVariables = X[nameOfNumericColumns].skew()\nfor index, value in enumerate (skewedVariables):\n    if (value > 1):\n        X[nameOfNumericColumns[index]]         = np.log (X[nameOfNumericColumns[index]]+0.001)\n        test_data[nameOfNumericColumns[index]] = np.log (test_data[nameOfNumericColumns[index]]+0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the columns with a greater percentage than desired of NaNs are dropped since they will not generalise well:\n\npercentageMissingValuesPerColumn = (len(X)-X.count())/len(X)*100\nmaxPercentageDesired = 0.05 # 5%\nfalseTrueColumns = percentageMissingValuesPerColumn>maxPercentageDesired\ncolumnIndexesToBeDropped = []\nfor i in range (X.shape[1]):\n    if (falseTrueColumns[i]): # True means that column has more percentage of NaNs than desired\n        columnIndexesToBeDropped.append(i)\nprint (X.shape)\nX_clean = X.drop(X.columns[columnIndexesToBeDropped], axis=1)\ntest_data_clean = test_data.drop(test_data.columns[columnIndexesToBeDropped], axis=1)\nallData = pd.concat([X_clean, test_data_clean])\nprint (allData.shape)\nallData_clean = pd.get_dummies(allData)\nprint (allData_clean.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variancePerColumn  = allData_clean.var( axis=0)\nmaximumPerColumn   = allData_clean.max( axis=0)\n\ncolumnsWorthKeeping = []\n\nfor i in range (len (variancePerColumn)):\n     if ((variancePerColumn[i]/maximumPerColumn[i])> 0.01):\n        columnsWorthKeeping.append(i)\nallData_clean = allData_clean.iloc[:,columnsWorthKeeping]\nprint (allData_clean.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = np.mean (allData_clean)\n#print (mean.shape)\nallData_clean = np.array(allData_clean.values)\n\n\nfor i in range (allData_clean.shape[0]):\n    for j in range (allData_clean.shape[1]):\n        if (np.isnan(allData_clean[i][j])):\n            allData_clean[i][j] = mean[j]\n\nX_clean = allData_clean[:1460]\ntest_data_clean = allData_clean[1460:]\n\nprint (X_clean.shape)\nprint (test_data_clean.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the X_clean data into train and validation sets. There are 1460 examples so 1200 / 260 seems reasonable:\nrandom_list = np.random.permutation(1460)\ncuttingNumber = 1200\n\nX_clean = np.array(X_clean)\nX_train = X_clean [random_list[:cuttingNumber]]\nY_train = Y       [random_list[:cuttingNumber]]\nX_val   = X_clean [random_list[cuttingNumber:]]\nY_val   = Y       [random_list[cuttingNumber:]]\ntest_data_clean = np.array(test_data_clean)\n\nprint (X_clean.shape)\nprint (Y.shape)\nprint (test_data_clean.shape)\nprint (IdTest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca = PCA (n_components=2)\nconcat = np.concatenate((X_train,X_val,test_data_clean),axis=0)\n\npca.fit(concat)\ntrainPoints = pca.transform(X_train)\nvalPoints = pca.transform(X_val)\ntestPoints = pca.transform (test_data_clean)\n\nplt.plot(trainPoints[:,0], trainPoints[:,1], c=\"red\")\nvalPoints = pca.transform(X_val)\nplt.plot(valPoints[:,0], valPoints[:,1], c=\"green\")\n\nplt.plot(testPoints[:,0], testPoints[:,1], c=\"blue\")\nplt.title (\"PCA: All three datasets belong to the same distribution\", fontweight=\"bold\")\nplt.xlabel(\"PCA 1\")\nplt.ylabel(\"PCA 2\")\nplt.legend([\"Training Points\",\"Validation Points\",\"Test Points\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nelastic = ElasticNet(random_state=1, max_iter=1000)\nparam = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\nelastic = GridSearchCV(elastic, param, cv=5, scoring='neg_mean_squared_error')\nelastic.fit(X_train,Y_train)\nprint (elastic.score(X_val,Y_val))\nprint('Elastic:', np.sqrt(elastic.best_score_*-1))\n\n\n\n\npredictions = elastic.predict(test_data_clean)\npredictions = np.expm1(predictions)\noutput = pd.DataFrame({'Id':IdTest,'SalePrice':predictions})\noutput.to_csv(\"my_submission.csv\", index=False)\nprint (\"Your submission was succesfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nimport tensorflow as tf\nfrom tensorflow import keras\ndef rmsle_error(y_true, y_pred): \n    return tf.math.sqrt(keras.losses.MSLE(y_true, y_pred))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Sometimes the model gets stuck because of a local optima around 12. Other times the gradients exploit. So we have to restart it\ninitialLoss = 100\nfinalLoss = np.nan\nES = keras.callbacks.EarlyStopping(baseline = 8,patience=20,min_delta=0.0001) # This spots those times when the model gets stuck in the first epoch\nwhile (initialLoss > 8 or (not finalLoss > 0)):\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(units=100, activation = \"relu\", input_shape = [X_train.shape[1],],kernel_initializer=keras.initializers.GlorotNormal()))\n    model.add(keras.layers.Dense(units=75, activation = \"relu\",kernel_initializer=keras.initializers.GlorotNormal()))\n    model.add(keras.layers.Dense(units=50, activation = \"relu\",kernel_initializer=keras.initializers.GlorotNormal()))\n    model.add(keras.layers.Dense(units=25, activation = \"relu\",kernel_initializer=keras.initializers.GlorotNormal()))\n    model.add(keras.layers.Dense(units=15, activation = \"relu\",kernel_initializer=keras.initializers.GlorotNormal()))\n    model.add(keras.layers.Dense(units=1,  activation = \"relu\",kernel_initializer=keras.initializers.GlorotNormal))\n    model.compile(optimizer = keras.optimizers.Adam(lr=0.0003), loss = rmsle_error)\n    history = model.fit (X_train, Y_train, batch_size=16, epochs = 400, callbacks=[ES], verbose=1, validation_data = (X_val,Y_val))\n    initialLoss = history.history[\"loss\"][0]\n    finalLoss = history.history[\"loss\"][-1]\n    print (model.evaluate(X_val, Y_val))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\npredictions = np.squeeze(model.predict(test_data_clean), axis=1)\noutput = pd.DataFrame({'Id':IdTest,'SalePrice':predictions})\noutput.to_csv(\"my_submission.csv\", index=False)\nprint (\"Your submission was succesfully saved!\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}