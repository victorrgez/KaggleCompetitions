{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this competition, a list of tweets must be classified as talking about a disaster or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is available publicly at https://www.kaggle.com/c/nlp-getting-started/data\n",
    "#### The model implementing BERT obtained a F1 score of 0.834 on the test dataset.\n",
    "#### An alternative model with whole-sentence tokenization (https://tfhub.dev/google/Wiki-words-250-with-normalization/2) obtained a significantly lower F1 score (0.789) and was discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References:\n",
    "\n",
    "###### -https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2: Bert model with instructions to instantiate the tokenizer and the Bert Keras Layer\n",
    "###### -https://github.com/tensorflow/models/blob/master/official/nlp/bert/tokenization.py: As explained in the previous link, the following class needs to be imported.\n",
    "###### -https://arxiv.org/abs/1810.04805: Original BERT paper that explains the uses of BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regarding the three inputs to the BERT LAYER:\n",
    "##### -Input_Word_Ids: This is just the tokenized version of the input with the pretrained embedding of BERT. The token [\"CLS\"] is added to the start of the sentence and the token [\"SEP\"] is appended at the end (for other applications, the model is trained with pairs of sequences, which are separated by [\"SEP\"].\n",
    "##### -Segment_Ids: BERT can work with pairs of phrases. In this case, \"0s\" would be assigned to tokens refering to the first phrase and \"1s\" to tokens related with the second phrase. When handling individual phrases, the same number will be allocated to all tokens.\n",
    "##### -Input_Mask: It depends on the source. Some authors claim that this is a positional embedding to be learnt by the model (the meaning of certain words change depending on their position in the phrase), wheras others claim that this is actually an Attention Mask which should have 1s in sentence tokens and 0s in the padding tokkens (given that all sentences are padded to the maximum length size). \n",
    "###### Both of these candidate inputs for the Input Mask were tried out and the latter obtained better results. It seems like the position is implicitly handled by the model and what Bert really needs to know is where the padding starts. According to https://github.com/google-research/bert/blob/master/run_classifier.py, \"The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# The libraries and the Tokenization class are imported:\n",
    "\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "import tokenization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Both train and test files are read into Pandas Dataframes. Only the text column is kept given that the tweet LOCATION\n",
    "#does not seem relevant and Bert does not need a KEYWORD for each tweet either:\n",
    "\n",
    "train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "\n",
    "Id_test = test_data[\"id\"] # This feature is written out to the output file.\n",
    "Y = train_data[\"target\"]\n",
    "\n",
    "train_data = train_data[\"text\"]\n",
    "test_data = test_data[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is really important to randomise the tweets' order before splitting them into training and validation sets given that their order might be alphabetical, having or non-having location first... or any other sort of non-random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6613 tweets have been allocated to the training dataset and 1000 tweets have been assigned to the validation dataset\n"
     ]
    }
   ],
   "source": [
    "cuttingNumber = 6613\n",
    "random_list = np.random.permutation(train_data.shape[0])\n",
    "\n",
    "train_X = train_data[random_list[:cuttingNumber]]\n",
    "val_X = train_data[random_list[cuttingNumber:]]\n",
    "test_X = test_data\n",
    "\n",
    "train_Y = Y[random_list[:cuttingNumber]]\n",
    "val_Y = Y[random_list[cuttingNumber:]]\n",
    "\n",
    "print (train_X.shape[0], \"tweets have been allocated to the training dataset and\", val_X.shape[0],\"tweets have been assigned to the validation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the BERT layer is imported and the tokenizer is instantiated according to \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\"\n",
    "max_seq_length = 128\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\",\n",
    "                            trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tweets into Bert's input.\n",
    "# Look at the third cell of the notebook to read the explanation of the three different Bert's inputs.\n",
    "# The \"bert_encode\" function at https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub has been adapted:\n",
    "\n",
    "def fromTweetsToBertInput(tweets, tokenizer, max_seq_length):\n",
    "    \n",
    "    input_word_ids = []\n",
    "    input_masks = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    for sentence in tweets:\n",
    "        \n",
    "        # The tokenizer splits each tweet into bert tokens (words and subwords). \n",
    "        # If the tweet is too long, it is truncated.\n",
    "        # The tokens [\"CLS\"] and [\"SEP\"] are appended as explained before.\n",
    "        \n",
    "        sentence = [\"[CLS]\"] + tokenizer.tokenize(sentence)[:max_seq_length-2] +[\"[SEP]\"]\n",
    "        \n",
    "        # All the inputs must be of the same size. Therefore, we need to do padding for short sentences. Also\n",
    "        #the tokens are converted into IDs (vectors of numbers) according to BERT pretrainned embedding:\n",
    "        \n",
    "        paddingLength = max_seq_length - len(sentence)\n",
    "        \n",
    "        input_word_id = tokenizer.convert_tokens_to_ids(sentence) + [0]*paddingLength\n",
    "        \n",
    "        # According to Google Research: \"The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        #tokens are attended to.\":\n",
    "        \n",
    "        input_mask = [1] * len(sentence) + [0] * paddingLength\n",
    "        \n",
    "        # Given that our model will handle single phrases, the same number (0s) is assigned to all tokens:\n",
    "        \n",
    "        segment_id = [0] * max_seq_length\n",
    "        \n",
    "        input_word_ids.append(input_word_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    input_word_ids = np.asarray(input_word_ids)\n",
    "    input_masks    = np.asarray(input_masks)\n",
    "    segment_ids    = np.asarray(segment_ids)\n",
    "    return input_word_ids, input_masks, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBertInput = fromTweetsToBertInput(train_X,tokenizer,max_seq_length)\n",
    "valBertInput = fromTweetsToBertInput(val_X,tokenizer,max_seq_length)\n",
    "testBertInput = fromTweetsToBertInput(test_X,tokenizer,max_seq_length)\n",
    "\n",
    "# Training and Validation sets are only joined together for the final Kaggle Score on the test set once the arquitecture\n",
    "#of the model has been previously defined with the validation set score on models trained only with the training set:\n",
    "\n",
    "trainPlusValBertInput = fromTweetsToBertInput(train_data,tokenizer,max_seq_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        keras_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Train on 7613 samples\n",
      "7613/7613 [==============================] - 431s 57ms/sample - loss: 0.4479 - binary_accuracy: 0.8052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ef9143704d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Bert Keras Model is defined (see the explanation of the three inputs in the third cell of this notebook):\n",
    "\n",
    "input_word_ids = keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "# Bert Layer has previously been imported:\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "# Here we can choose the output we want to use from the Bert layer. The \"pooled output\"contains information of the \n",
    "#whole sentence (the output of all tokens is sintetized together). The \"sequence output\" contains the individual\n",
    "#token vectors which can be processed with LSTMs, GRUs and other types of RNNs. For the purpose of this notebook,\n",
    "#the Bert Model already needs more than 30 million parameters and the kaggle kernel with GPU is already working\n",
    "#near its full capacity, so the \"pooled output\" was chosen.\n",
    "\n",
    "# NOTE that if we increase the batch_size beyond 8, the notebook will crash because of the size of the model!\n",
    "\n",
    "# One single neuron with a sigmoid activation function is enough to get decent scores without incrementing the\n",
    "#number of parameters. If we train the model for more than one epoch, it will overfit the training set:\n",
    "\n",
    "disasterOrNot = keras.layers.Dense(units = 1, activation = \"sigmoid\" ) (pooled_output)\n",
    "\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=disasterOrNot)\n",
    "model.summary()\n",
    "model.compile(loss= \"binary_crossentropy\", optimizer=keras.optimizers.Adam(lr=1e-5), metrics=[\"binary_accuracy\"])\n",
    "model.fit(trainPlusValBertInput, Y, epochs = 1, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F1 score is used in the Kaggle competition. It takes into account the precision and the recall:\n",
    "\n",
    "def f1Score (Y_true, Y_pred):\n",
    "    TP = len (Y_true[Y_true==1][Y_true==Y_pred])\n",
    "    FP = len (Y_true[Y_true==0][Y_true!=Y_pred])\n",
    "    FN = len (Y_true[Y_true==1][Y_true!=Y_pred])\n",
    "    TN = len (Y_true[Y_true==0][Y_true==Y_pred])\n",
    "    PC = TP/(TP+FP)\n",
    "    RC = TP/(TP+FN)\n",
    "    F1 = (2*PC*RC)/(PC+RC)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalPredictions = np.squeeze(model.predict(valBertInput), axis=1)\\nvalPredictions = (valPredictions>0.5).astype(int)\\nprint (f1Score(val_Y, valPredictions))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell prints the F1 score on the validation set but it has been commented out given that for the final\n",
    "#submission, the model has been trained on both training and validation dataset.\n",
    "\n",
    "\"\"\"\n",
    "valPredictions = np.squeeze(model.predict(valBertInput), axis=1)\n",
    "valPredictions = (valPredictions>0.5).astype(int)\n",
    "print (f1Score(val_Y, valPredictions))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "predictions = np.squeeze(model.predict(testBertInput), axis=1)\n",
    "predictions = (predictions>0.5).astype(int)\n",
    "output = pd.DataFrame({'id': Id_test, 'target': predictions})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
